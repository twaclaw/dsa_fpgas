<!doctype html>
<html>

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

	<title>fpgas</title>

	<link rel="stylesheet" href="revealjs/dist/reset.css">
	<link rel="stylesheet" href="revealjs/dist/reveal.css">
	<link rel="stylesheet" href="revealjs/dist/theme/black.css" id="theme">

	<!-- Theme used for syntax highlighted code -->
	<link rel="stylesheet" href="revealjs/plugin/highlight/monokai.css" id="highlight-theme">
	<style>
		div.footer {
			margin-top: 600px;
			font-size: 20px;
		}

		.reveal h1,
		.reveal h2,
		.reveal h3,
		.reveal h4,
		.reveal h5 {
			text-transform: none;
		}
	</style>
</head>

<body>
	<div class="reveal">
		<div class="slides">
			<!-- ----- -->
			<!-- Beginning of slides -->
			<!-- ----- -->
			<section>
				<h2>Acceleration of algorithms with FPGAs 101</h2>
				<p>Diego Sandoval</p>
				<p>2021</p>
			</section>

			<section>
				<!-- Motivation -->
				<section>
					<h2>Motivation:</h2>
					<h3 class="fragment" style="color:darksalmon">Need for speed</h3>
				</section>
				<section data-markdown>
					<textarea data-template>
						## <span style="color:darksalmon">Why</span> is acceleration required?
						* Latency
						* Throughput
						* Reducing costs
						* Reducing energy consumption
						* Increasing productivity
					</textarea>
				</section>

				<section data-markdown>
					<textarea data-template>
						<!-- ## <span style="color:darksalmon">Where</span> is it needed? -->
						## In which domains?
						* AI
						* Computer vision
						* HPC
							* Genomics
							* Seismic imaging
						* Finance
						* Security
						* Database 
						<!-- (Graph DB, BigData queries, regex processing) -->
						* ...
						
					</textarea>
				</section>

				<section data-markdown>
					<textarea data-template>
						## <span style="color:darksalmon">How</span> to achieve acceleration?
						* <span class="fragment strike">A faster üêé: increasing the clock frequency</span> üëé
							<!-- *	Power performance dynamic: $\uparrow f_{\text{clk}} \sim \text{power}^{3}$ [[Intel]](https://software.intel.com/content/www/us/en/develop/articles/vectorization-a-key-tool-to-improve-performance-on-modern-cpus.html) -->
						* <span class="fragment highlight-green">Parallelism</span> üëç
						* Using a "custom", specialized, <span class="fragment highlight-green">Domain Specific Architecture (DSA)</span> üëç
					</textarea>
				</section>

				<section data-markdown>
					<textarea data-template>
					### Parallelism 

					* [Data Parallelism](https://en.wikipedia.org/wiki/Data_parallelism): 
					Doing the same task on different components of data. SIMD, GPUs.
					* [Task Parallelism](https://en.wikipedia.org/wiki/Task_parallelism):
					 Doing different tasks at the same time. Concurrency. Pipelining. 
					</textarea>
				</section>
				<!-- 
				<section data-background-color="#228b22">
					<h2 class="fragment strike">Domain Specific Architectures (DAS)</h2>
				</section> -->

				<section data-markdown>
					<textarea data-template>
					### Domain Specific Architectures (DSA)
					* ASICs (e.g. TPUs)
					* FPGAs						
					</textarea>
				</section>
			</section>

			<section>
				<!-- How to use fpgas to accelerate  -->
				<section data-markdown data-background-color="#EEE8AA">
					<textarea data-template>
						## Building accelerators with FPGAs
					</textarea>
				</section>
				<section data-markdown data-background-color="#FFFFFF">
					<textarea data-template>
						## <span style="color:darksalmon">What</span> is an FPGA?
						<span class="fragment highlight-red">Field Programmable</span> Generic Array
						<ul class="fragment fade-in">
							<li>A 32-bit processor (capable of running GNU/Linux)</li>
							<li>A software-defined radio</li>
							<li>A DPU (Deep Learning Processing Unit)</li>
							<li>A video encoder</li>
							<li>A spectrometer</li>
							<li>An implementation of any computable problem</li>
						</ul>
					</textarea>
				</section>

				<section data-markdown data-background-color="#FFFFFF">
					<textarea data-template>
					## Basic unit: LUT

					<img data-src="images/fpga-lut.png">

					[Source: pp4fpgas](http://kastner.ucsd.edu/hlsbook/)
					</textarea>
				</section>

				<section data-markdown data-background-color="#FFFFFF">
					<textarea data-template>
					## Slices and switchboxes 

					<img data-src="images/fpga-slice.png">

					[Source: pp4fpgas](http://kastner.ucsd.edu/hlsbook/)
					</textarea>
				</section>

				<section data-markdown data-background-color="#FFFFFF">
					<textarea data-template>
					## Slices and switchboxes 

					<img data-src="images/fpga-structure.png" height="400">

					[Source: pp4fpgas](http://kastner.ucsd.edu/hlsbook/)
					</textarea>
				</section>

				<section data-markdown data-background-color="#FFFFFF">
					<textarea data-template>
					## Hardened elements

					<img data-src="images/fpga-processor.png">

					[Source: pp4fpgas](http://kastner.ucsd.edu/hlsbook/)
					</textarea>
				</section>

				<section data-markdown data-background-color="#FFFFFF">
					<textarea data-template>
					## Memory  hierarchy
					<table style="font-size:20pt;">
						<thead>
							<tr>
								<th></th>
								<th>External memory</th>
								<th>BRAMs</th>
								<th>FFs</th>
							</tr>
						</thead>
						<tbody>
							<tr>
								<td>count</td>
								<td>1-4</td>
								<td>thousands</td>
								<td>millions</td>
							</tr>
							<tr>
								<td>size</td>
								<td>GBytes</td>
								<td>KBytes</td>
								<td>Bits</td>
							</tr>
							<tr>
								<td>total size</td>
								<td>GBytes</td>
								<td>MBytes</td>
								<td>100s of KBytes</td>
							</tr>
							<tr>
								<td>width</td>
								<td>8-64</td>
								<td>1-16</td>
								<td>1</td>
							</tr>
							<tr>
								<td>total bandwidth</td>
								<td>GBytes/s</td>
								<td>TBytes/s</td>
								<td>100s of TBytes/s</td>
							</tr>
						</tbody>
					</table>

					[Source: pp4fpgas](http://kastner.ucsd.edu/hlsbook/)
					</textarea>
				</section>
				<section data-background-image="images/coffee.jpg">
					<h1>?</h1>
				</section>
			</section>

			<section>
				<!-- How to use fpgas to accelerate  -->
				<section data-markdown data-background-color="#FF69B4">
					<textarea data-template>
						## Demo
					</textarea>
				</section>

				<section data-markdown>
					<textarea data-template>
					<img data-src="images/pynq-z1.png" height="500">

					[Source: Digilent](https://reference.digilentinc.com/reference/programmable-logic/pynq-z1/reference-manual)
					</textarea>
				</section>
				<section data-markdown>
					<textarea data-template>
						* <span style="color:darkorange">650MHz</span> dual-core Cortex-A9 processor
						<!-- * DDR3 memory controller with 8 DMA channels -->
						* Artix-7 familiy programmable logic
							* 13300 logic slices, each with four 6-input LUTS and 8 flip-flops
							* 630 KB of fast block RAM
							* 220 DSP slices
							<!-- * On-chip XADC -->
							* 4 clock management tiles, each with a phase-locked loop (PLL) and mixed-mode clock manager (MMCM)
								* Running at <span style="color:darkorange">100MHz</span>


						[Source: Digilent](https://reference.digilentinc.com/reference/programmable-logic/pynq-z1/reference-manual)
					</textarea>
				</section>
				<section data-markdown>
					<textarea data-template>
					<img data-src="images/pynq-z1-block.png" height="500">

					[Source: Digilent](https://reference.digilentinc.com/reference/programmable-logic/pynq-z1/reference-manual)
					</textarea>
				</section>



				<section data-markdown>
					<textarea data-template>
						## Examples

						*	[PYNQ Hello World: image resizing](https://github.com/Xilinx/PYNQ-HelloWorld)
						*   [Binarized/Quantized NN: CIFAR image recognition](https://github.com/Xilinx/BNN-PYNQ)
					</textarea>
				</section>
			</section>

			<section>
				<section data-markdown data-background-color="#20B2AA">
					<textarea data-template>
						## Breaking free from architectural restrictions
					</textarea>
				</section>
	
			<section data-markdown>
					<textarea data-template>
						## <span style="color:darksalmon">What</span> is the main limitation?
						<img data-src="images/fetch1.png" height="200">

						* Instruction Fetch (IF)
						* Instruction Decode (ID)
						* Execution
						* Memory Operations
						* Write Back (WB)

						[Source: Intro to HLS](https://www.xilinx.com/support/documentation/sw_manuals/xilinx2018_3/ug902-vivado-high-level-synthesis.pdf)
					</textarea>
				</section>

				<section data-markdown>
					<textarea data-template>
						## <span style="color:darksalmon">What</span> is the main limitation?

						<pre><code class="c" data-trim data-noescape>
							z = a + b;
						</code></pre>
					
						<pre><code class="arm" data-trim data-noescape>
							LD		a, $R1
							LD		b, $R2
							ADD  	 $R1,$R2,$R3
							ST		$R3, c
						</code></pre>

						[Source: Intro to HLS](https://www.xilinx.com/support/documentation/sw_manuals/xilinx2018_3/ug902-vivado-high-level-synthesis.pdf)
					</textarea>
				</section>


				<section data-markdown>
					<textarea data-template>
						## <span style="color:darksalmon">What</span> is the main limitation?

						<img data-src="images/fetch2.png">

						[Source: Intro to HLS](https://www.xilinx.com/support/documentation/sw_manuals/xilinx2018_3/ug902-vivado-high-level-synthesis.pdf)
					</textarea>
				</section>
				<section data-markdown>
					<textarea data-template>
						## <span style="color:darksalmon">What</span> are FPGAs good at?
						<img data-src="images/fpga-exec.png" height="400">

						[Source: Intro to HLS](https://www.xilinx.com/support/documentation/sw_manuals/xilinx2018_3/ug902-vivado-high-level-synthesis.pdf)
					</textarea>
				</section>

				<section data-markdown>
					<textarea data-template>
						<!-- ### <span style="color:darksalmon">Who</span> does offer FPGA-based services?							 -->
						### FPGA-based services are relatively new
						* Intel (Altera)
							* Microsoft
						* Both Intel and Xilinx
							* Alibaba Cloud, Nimbix
						* Xilinx
							* AWS
							* Baidu
							* Huawei
							* Tencent Cloud
					</textarea>
				</section>

<!-- 
				<section data-markdown>
					<textarea data-template>
						## <span style="color:darksalmon">Why</span> didn't it happen earlier?

						* Mismatch between software and hardware development worlds
						* RTL languages (VHDL, Verilog) very different from C, C++
						* Not as many hardware engineers as software developers
						* Very high engineering costs
					</textarea>
				</section>


				<section data-markdown>
					<textarea data-template>
					## <span style="color:darksalmon">Why</span> didn't it happen earlier?
					#### Example: edge detector implemented in Verilog

					<pre><code class="verilog" data-trim data-noescape>
					module edgeDetector
					(
						input wire clk, reset, 
						input wire level, 
						output reg Mealy_tick, Moore_tick
					);
						
					localparam [1:0] // 3 states are required for Moore
						zeroMoore = 2'b00,
						edgeMoore = 2'b01, 
						oneMoore = 2'b10;
						
					reg[1:0] stateMoore_reg, stateMoore_next;
						
					always @(posedge clk, posedge reset)
					begin
						if(reset) // go to state zero if rese
							begin
							stateMoore_reg <= zeroMoore;
							end
						else // otherwise update the states
							begin
							stateMoore_reg <= stateMoore_next;
							end
					end	

					// Moore Design 
					always @(stateMoore_reg, level)
					begin
						// store current state as next
						stateMoore_next = stateMoore_reg; // required: when no case statement is satisfied

						Moore_tick = 1'b0; // set tick to zero (so that 'tick = 1' is available for 1 cycle only)
						case(stateMoore_reg)
							zeroMoore: // if state is zero,
								if(level) // and level is 1
									stateMoore_next = edgeMoore; // then go to state edge.
							edgeMoore:
								begin
									Moore_tick = 1'b1; // set the tick to 1.
									if(level) // if level is 1, 
										stateMoore_next = oneMoore; // go to state one,
									else    
										stateMoore_next = zeroMoore; // else go to state zero.
								end
							oneMoore: 
								if(~level) // if level is 0,
									stateMoore_next = zeroMoore; // then go to state zero.      
						endcase
					end
					endmodule
					</code></pre>

					[Source: verilogguide](https://verilogguide.readthedocs.io/en/latest/verilog/fsm.html)
					</textarea>
				</section> -->

				<section data-markdown>
					<textarea data-template>
						## <span style="color:darksalmon">Why</span> didn't it happen earlier?
						<img data-src="images/time-to-market.png" height="400">

						[Source: Intro to HLS](https://www.xilinx.com/support/documentation/sw_manuals/xilinx2018_3/ug902-vivado-high-level-synthesis.pdf)
					</textarea>
				</section>

				<section data-markdown data-background-color="#20B2AA">
					<textarea data-template>
						## High Level Synthesis (HLS)
						### It is like compiling for an architecture that doesn't yet exist!
					</textarea>
				</section>
	
				<section data-markdown>
					<textarea data-template>
						## High Level Synthesis (HLS)

						<img data-src="images/compile.png" height="480">

						[Source: Intro to HLS](https://www.xilinx.com/support/documentation/sw_manuals/xilinx2018_3/ug902-vivado-high-level-synthesis.pdf)
	
					</textarea>
				</section>

				<section data-markdown>
					<textarea data-template>
						## HLS

						<img data-src="images/time-to-market-2.png" height="400">

						[Source: Intro to HLS](https://www.xilinx.com/support/documentation/sw_manuals/xilinx2018_3/ug902-vivado-high-level-synthesis.pdf)
					</textarea>
				</section>
				<section data-background-image="images/coffee.jpg">
					<h1>?</h1>
				</section>
			
			</section>

			<section>
				<!-- Example mmult HLS -->
				<section data-markdown data-background-color="#F5FFFA">
					<textarea data-template>
						## <span style="color:darksalmon">How</span> to do it?
						### Example of kernel optimization
					</textarea>
				</section>

				<section data-markdown>
					<textarea data-template>
						## C, C++ HLS limitations
						#### Constructs that don't make sense in hardware

						* System calls (e.g. `getc()`, `time()`)
						* Dynamic memory
						* Some pointers operations are not supported (e.g. arrays of pointers cannot point to additional pointers)
						* Recursive functions

						[Source: Vivado HLS](https://www.xilinx.com/support/documentation/sw_manuals/xilinx2018_3/ug902-vivado-high-level-synthesis.pdf)
					</textarea>
				</section>

				<section data-markdown>
					<textarea data-template>
						## C, C++ HLS advantages
						### HLS arbitrary precision types

						<pre><code class="cpp" data-trim data-noescape>
						#include "ap_int.h"

						//ap[u]int< W>

						# define ap_uint<512> uint512_t;

						ap_int<57> my_type;
						ap_uint<1> bit = 0;
						</code></pre>

						<pre><code class="cpp" data-trim data-noescape>
						#include "ap_fixed.h"

						ap_ufixed<18,6,AP_RND> my_fixed_type;
						</code></pre>

					</textarea>
				</section>

				<!-- Example matrix multiplication -->

				<section data-markdown>
					<textarea data-template>
						#### Example: matrix multiplication $\mathbf{C}=\mathbf{A}\mathbf{B}$
						##### $\mathbf{A}, \mathbf{B}, \mathbf{C} \in \mathbb{R}^{128\text{x}128}$

						<pre><code class="cpp" data-trim data-noescape>
							template <typename T> void kernel_mmult(T a[N][N], T b[N][N], T out[N][N]) {
							L1:for (int m = 0; m < N; ++m) {
								L2:for (int n = 0; n < N; ++n) {
									T sum = 0;
									L3:for (int k = 0; k < N; ++k)
										sum += a[m][k] * b[k][n];

									out[m][n] = sum;
								}
								}
								return;
							}
						</code></pre>

						[Source: github:twaclaw/matmult](https://github.com/twaclaw/matmult)
					</textarea>
				</section>

				<section data-markdown>
					<textarea data-template>
						<pre><code class="txt" data-trim data-noescape>

	+ Latency: 
	* Summary: 
	+----------+----------+-----------+-----------+----------+----------+---------+
	|   Latency (cycles)  |   Latency (absolute)  |       Interval      | Pipeline|
	|    min   |    max   |    min    |    max    |    min   |    max   |   Type  |
	+----------+----------+-----------+-----------+----------+----------+---------+
	|  25198849|  25198849|  0.252 sec|  0.252 sec|  25198850|  25198850|     none|
	+----------+----------+-----------+-----------+----------+----------+---------+


	================================================================
	== Utilization Estimates
	================================================================
	* Summary: 
	+-----------------+---------+-----+--------+-------+-----+
	|       Name      | BRAM_18K| DSP |   FF   |  LUT  | URAM|
	+-----------------+---------+-----+--------+-------+-----+
	|DSP              |        -|    -|       -|      -|    -|
	|Expression       |        -|    -|       0|    140|    -|
	|FIFO             |        -|    -|       -|      -|    -|
	|Instance         |        -|    3|     455|   1042|    -|
	|Memory           |        -|    -|       -|      -|    -|
	|Multiplexer      |        -|    -|       -|    101|    -|
	|Register         |        -|    -|     207|      -|    -|
	+-----------------+---------+-----+--------+-------+-----+
	|Total            |        0|    3|     662|   1283|    0|
	+-----------------+---------+-----+--------+-------+-----+
	|Available        |      280|  220|  106400|  53200|    0|
	+-----------------+---------+-----+--------+-------+-----+
	|Utilization (%)  |        0|    1|      ~0|      2|    0|
	+-----------------+---------+-----+--------+-------+-----+
						</code></pre>
					</textarea>
				</section>

				<section data-markdown>
					<textarea data-template>
						## Optimization 1: pipelining 
						#### Pipelining of L3 is automatic

						<pre><code class="cpp" data-trim data-noescape>
							template <typename T> void kernel_mmult(T a[N][N], T b[N][N], T out[N][N]) {
							L1:for (int m = 0; m < N; ++m) {
								L2:for (int n = 0; n < N; ++n) {
							#pragma HLS PIPELINE II = 1
									T sum = 0;
									L3:for (int k = 0; k < N; ++k)
										sum += a[m][k] * b[k][n];

									out[m][n] = sum;
								}
								}
								return;
							}
						</code></pre>

						<!-- [[github:twaclaw/matmult]](https://github.com/twaclaw/matmult) -->
					</textarea>
				</section>
				<section data-markdown>
					<textarea data-template>
						## <span style="color:darksalmon">What</span> is pipelining?
						#### `y = (a * x) + b + c`
						<img data-src="images/pipelining1.png" height="400">

						[Source: Intro to HLS](https://www.xilinx.com/support/documentation/sw_manuals/xilinx2018_3/ug902-vivado-high-level-synthesis.pdf)
					</textarea>
				</section>
				<section data-markdown>
					<textarea data-template>
						## <span style="color:darksalmon">What</span> is pipelining?
						### Initialization Interval II

						<img data-src="images/pipelining2.png">
						
						[Source: Intro to HLS](https://www.xilinx.com/support/documentation/sw_manuals/xilinx2018_3/ug902-vivado-high-level-synthesis.pdf)
					</textarea>
				</section>
			
				<section data-markdown>
					<textarea data-template>
						<pre><code class="txt" data-trim data-noescape>
	+ Latency: 
    * Summary: 
    +---------+---------+-----------+-----------+---------+---------+---------+
    |  Latency (cycles) |   Latency (absolute)  |      Interval     | Pipeline|
    |   min   |   max   |    min    |    max    |   min   |   max   |   Type  |
    +---------+---------+-----------+-----------+---------+---------+---------+
    |  1049161|  1049161|  10.492 ms|  10.492 ms|  1049162|  1049162|     none|
    +---------+---------+-----------+-----------+---------+---------+---------+


	================================================================
	== Utilization Estimates
	================================================================
	* Summary: 
	+-----------------+---------+-----+--------+-------+-----+
	|       Name      | BRAM_18K| DSP |   FF   |  LUT  | URAM|
	+-----------------+---------+-----+--------+-------+-----+
	|DSP              |        -|    -|       -|      -|    -|
	|Expression       |        -|    -|       -|      -|    -|
	|FIFO             |        -|    -|       -|      -|    -|
	|Instance         |        -|   10|   12673|   9863|    -|
	|Memory           |        -|    -|       -|      -|    -|
	|Multiplexer      |        -|    -|       -|     14|    -|
	|Register         |        -|    -|       3|      -|    -|
	+-----------------+---------+-----+--------+-------+-----+
	|Total            |        0|   10|   12676|   9877|    0|
	+-----------------+---------+-----+--------+-------+-----+
	|Available        |      280|  220|  106400|  53200|    0|
	+-----------------+---------+-----+--------+-------+-----+
	|Utilization (%)  |        0|    4|      11|     18|    0|
	+-----------------+---------+-----+--------+-------+-----+
							</code>
							</pre>
					</textarea>
				</section>

				<section data-markdown>
					<textarea data-template>
						## <span style="color:darksalmon">What</span> is array partitioning?

						<img data-src="images/partitioning.png">
						
						[[DiCecco]](https://tspace.library.utoronto.ca/bitstream/1807/89598/3/DiCecco_Roberto_201806_MAS_thesis.pdf)
					</textarea>
				</section>

				<section data-markdown>
					<textarea data-template>
						## Optimization 2: array partitioning 

						<pre><code class="cpp" data-trim data-noescape>
							template <typename T> void kernel_mmult(T a[N][N], T b[N][N], T out[N][N]) {

							#pragma HLS ARRAY_PARTITION variable = a factor = 16 dim = 2 block
							#pragma HLS ARRAY_PARTITION variable = b factor = 16 dim = 1 block

							L1:for (int m = 0; m < N; ++m) {
								L2:for (int n = 0; n < N; ++n) {
							#pragma HLS PIPELINE II = 1
									T sum = 0;
									L3:for (int k = 0; k < N; ++k)
										sum += a[m][k] * b[k][n];

									out[m][n] = sum;
								}
								}
								return;
							}
						</code></pre>

						<!-- [[github:twaclaw/matmult]](https://github.com/twaclaw/matmult) -->
					</textarea>
				</section>

				<section data-markdown>
					<textarea data-template>
						<pre><code class="txt" data-trim data-noescape>
	+ Latency: 
	* Summary: 
	+---------+---------+----------+----------+-------+-------+---------+
	|  Latency (cycles) |  Latency (absolute) |    Interval   | Pipeline|
	|   min   |   max   |    min   |    max   |  min  |  max  |   Type  |
	+---------+---------+----------+----------+-------+-------+---------+
	|    66180|    66180|  0.662 ms|  0.662 ms|  66180|  66180|     none|
	+---------+---------+----------+----------+-------+-------+---------+
						
						
	================================================================
	== Utilization Estimates
	================================================================
	* Summary: 
	+-----------------+---------+-----+--------+-------+-----+
	|       Name      | BRAM_18K| DSP |   FF   |  LUT  | URAM|
	+-----------------+---------+-----+--------+-------+-----+
	|DSP              |        -|    -|       -|      -|    -|
	|Expression       |        -|    -|       0|    189|    -|
	|FIFO             |        -|    -|       -|      -|    -|
	|Instance         |        -|  160|   11136|  22752|    -|
	|Memory           |        -|    -|       -|      -|    -|
	|Multiplexer      |        -|    -|       -|   4900|    -|
	|Register         |        -|    -|   25021|   4160|    -|
	+-----------------+---------+-----+--------+-------+-----+
	|Total            |        0|  160|   36157|  32001|    0|
	+-----------------+---------+-----+--------+-------+-----+
	|Available        |      280|  220|  106400|  53200|    0|
	+-----------------+---------+-----+--------+-------+-----+
	|Utilization (%)  |        0|   72|      33|     60|    0|
	+-----------------+---------+-----+--------+-------+-----+
						
							</code></pre>
					</textarea>
				</section>

				<section data-markdown>
					<textarea data-template>
						## <span style="color:darksalmon">How</span> to build?
						### Vitis_HLS (Vivado_HLS)

						<pre><code class="tcl" data-trim data-noescape>
							open_project matmult
							set_top matmult_accel

							add_files matmult.h

							add_files matmult_accel.cpp \
							-cflags "-D__SDSVHLS__ -std=c++0x"

							add_files -tb matmult_tb.cpp \
							-cflags "-D__SDSVHLS__ -std=c++0x"

							open_solution "solution1"
							set_part {zynq}
							create_clock -period 10 -name default
							set_clock_uncertainty 27.0%

							# set_directive_pipeline -II 1 "mmult_hw/L2"
							# set_directive_array_partition -type block -factor 16 -dim 2 "mmult_hw" a
							# set_directive_array_partition -type block -factor 16 -dim 1 "mmult_hw" b

							# run testbench
							csim_design -clean 

							csynth_design
							export_design -format ip_catalog -description "Matrix multiplication IP" -display_name "matmult_accel"
							exit
						</code></pre>

						<pre><code class="bash">
						$ vitis_hls script.tcl
						</code></pre>

					</textarea>
				</section>

				<section data-markdown>
					<textarea data-template>
						## <span style="color:darksalmon">How</span> to build?
						<img data-src="images/matmult.png">
					</textarea>
				</section>

				<section data-markdown>
					<textarea data-template>
						## <span style="color:darksalmon">How</span> to interact with the accelerator?
						<pre><code class="python" data-trim data-noescape>

							from pynq import (allocate, Overlay)
							import numpy as np

							# programs the FPGA
							overlay = Overlay('./matmult.bit')

							dma = overlay.dma
							mmult_ip = overlay.accel

							# allocates memory for the DMA transfers
							DIM = 128
							in_buffer = allocate(shape=(2, DIM, DIM), dtype=np.float32, cacheable=False)
							out_buffer = allocate(shape=(DIM, DIM), dtype=np.float32, cacheable=False)

							CTRL_REG = 0x00
							AP_START = (1<<0) # bit 0
							AUTO_RESTART = (1<<7) # bit 7

							def run_kernel():
							    dma.sendchannel.transfer(in_buffer)
							    dma.recvchannel.transfer(out_buffer)
							    mmult_ip.write(CTRL_REG, (AP_START | AUTO_RESTART))  # initialize the module
							    dma.sendchannel.wait()
								dma.recvchannel.wait()

							# creates example matrices to evaluate the kernel
							A = np.random.rand(DIM, DIM).astype(dtype=np.float32)
							B = np.random.rand(DIM, DIM).astype(dtype=np.float32)

							in_buffer[:] = np.stack((A, B))

							run_kernel()

							# verify correctness, Numpy is the golden standard
							np.array_equal(A @ B, out_buffer)

						</code></pre>	

						Check out the [results](https://github.com/twaclaw/matmult/blob/master/notebooks/matmult/matmult.ipynb)
					</textarea>
				</section>

				<section data-markdown>
					<textarea data-template>
						## Additional [optimizations](https://www.xilinx.com/html_docs/xilinx2020_2/vitis_doc/vitis_hls_optimization_techniques.html)
						### Dataflow

						<pre><code class="txt">
							_____________
							|             |<----- Input Vector from Global Memory
							|  read_input |       __
							|_____________|----->|  |
							 _____________       |  | inStream
							|             |<-----|__|
							| compute_add |       __
							|_____________|----->|  |
							 _____________       |  | outStream
							|              |<----|__|
							| write_result |
							|______________|-----> Output result to Global Memory
		
						</code></pre>
						[Source: github:xilinx/Vitis_Accel_Examples](https://github.com/Xilinx/Vitis_Accel_Examples/tree/master/cpp_kernels/dataflow_stream)
					</textarea>
				</section>

				<section data-markdown>
					<textarea data-template>
						<pre><code class="cpp" data-noescape data-trim>
							#include "hls_stream.h"

							static hls::stream< unsigned int> inStream("input_stream");
							static hls::stream< unsigned int> outStream("output_stream");
							#pragma HLS STREAM variable = inStream depth = 32
							#pragma HLS STREAM variable = outStream depth = 32
							
							#pragma HLS dataflow
							read_input(in, inStream, size);
							compute_add(inStream, outStream, inc, size);
							write_result(out, outStream, size);
						</code></pre>
					</textarea>
				</section>
			
				<section data-markdown>
					<textarea data-template>
						## Additional [optimizations](https://www.xilinx.com/html_docs/xilinx2020_2/vitis_doc/vitis_hls_optimization_techniques.html)
						###  Interface optimizations
						<pre><code class="cpp" data-noescape data-trim>
							void mmult(uint512_t * a, uint512_t * b, uint512_t * c)
							{
						  #pragma HLS INTERFACE s_axilite port = return bundle = control
						  #pragma HLS INTERFACE s_axilite port = a bundle = control
						  #pragma HLS INTERFACE s_axilite port = b bundle = control
						  #pragma HLS INTERFACE s_axilite port = c bundle = control
						  #pragma HLS INTERFACE m_axi port = a bundle = gemm_a 
						  #pragma HLS INTERFACE m_axi port = b bundle = gemm_b 
						  #pragma HLS INTERFACE m_axi port = c bundle = gemm_c 

						</code></pre>
					</textarea>
				</section>


				<section data-background-image="images/coffee.jpg">
					<h1>?</h1>
				</section>
			</section>

			<section>
				<!-- General methodology  -->
				<section data-markdown data-background-color="#FFEBCD">
					<textarea data-template>
						## General acceleration methodology 
						### An overview
					</textarea>
				</section>

				<section data-background-color="#FFEBCD">
					<h2>Arithmetic Intensity $\text{I}$ </h2>

						\[\begin{aligned}
						\text{FLOPS} &amp; = \frac{\text{#FLOP}}{\text{time(s)}}\\
						\\
						 &amp; = \frac{\text{#FLOP}}{\text{Byte}} \cdot \frac{\text{Byte}}{\text{s}} \\
						 \\
						 &amp; = \text{I} \cdot \text{BW} \\
						\end{aligned} \]
				</section>

				<section data-markdown data-background-color="#FFEBCD">
					<textarea data-template>
						## Arithmetic Intensity
						<table>
							<thead>
								<tr>
									<th></th>
									<th>$\text{I}$</th>
								</tr>
							</thead>
							<tbody>
								<tr>
									<td>
										Matrix addition
									</td>
									<td>
										$\frac{N^2}{3N^2} = \frac{1}{3}$
									</td>
								</tr>
								<tr>
									<td>
										Matrix multiplication
									</td>
									<td>
										$\frac{(2N-1)N^{2}}{3N^2} = \frac{2N-1}{3}$
									</td>
								</tr>
							</tbody>
						</table>
					</textarea>
				</section>

				<section data-background-color="#FFEBCD">
					<h2>Arithmetic Intensity ($\text{I}$) </h2>
						\[\begin{aligned}
						 \text{FLOPS} &amp; = \text{I} \cdot \text{BW} \\
						\\
						 \log(\text{FLOPS}) &amp; = \log(\text{I} \cdot \text{BW}) \\
						 \\
						 \log(\text{FLOPS}) &amp; = \log(\text{I}) + \log(\text{BW}) \\
						\end{aligned} \]
				</section>
				<section data-markdown data-background-color="#FFEBCD">
					<textarea data-template>
						## Roofline diagram
						<img data-src="images/roofline0.png">

						[[S. Williams 2008](https://www2.eecs.berkeley.edu/Pubs/TechRpts/2008/EECS-2008-164.html), [Wikipedia](https://en.wikipedia.org/wiki/Roofline_model)]
					</textarea>
				</section>
				<section data-markdown data-background-color="#FFEBCD">
					<textarea data-template>
					## Acceleration Guideline

					* Profile your application
					* Decide which part of the algorithm to offload to the FPGA		
					* Optimize the kernel (loop unrolling, pipelining, dataflow, etc.)
					* Optimize the interface
					* Optimize the host application
					</textarea>
				</section>

			</section>
			<section>
				<section data-markdown data-background-color="#DDA0DD">
					<textarea data-template>
						## A final example:
						### Vitis and AWS instances
					</textarea>
				</section>
				<section data-markdown>
					<textarea data-template>
					<img data-src="images/vitis.png">
					
					[Source: Xilinx](https://www.xilinx.com/products/design-tools/vitis/vitis-platform.html)
					</textarea>
				</section>

				<section data-markdown>
					<textarea data-template>
						## Building with Vitis flow
						You don't need an FPGA instance for building

						<pre><code class="bash" data-noescape data-trim>
							# creates a  .xclbin
							make TARGET={sw_emu,hw_emu,hw} DEVICE=$AWS_PLATFORM all

							# creates an .awsxclbin
							${VITIS_TOOLS}/create_vitis_afi.sh  -xclbin=./mmult.xclbin\
							  -o=mmult -s3_bucket=vitiss3\
							  -s3_dcp_key=mmult\
							  -s3_logs_key=mmult
						</code></pre>
					</textarea>
				</section>

			

				<section data-markdown>
					<textarea data-template>
						#### [AWS Example](https://github.com/Xilinx/Vitis-AWS-F1-Developer-Labs/tree/master/modules/module_03): Gaussian Blur
						<img data-src="images/conv.png" height="400">

						[Source image: Wikipedia](https://en.wikipedia.org/wiki/Gaussian_blur)
					</textarea>
				</section>
				<section data-markdown>
					<textarea data-template>
						## Further reading

						* [Vitis](https://www.xilinx.com/products/design-tools/vitis/vitis-platform.html)
						* [https://github.com/aws/aws-fpga](https://github.com/aws/aws-fpga)
						* [How to build a kernel for AWS](https://github.com/aws/aws-fpga/blob/master/Vitis/README.md)
						* [https://github.com/Xilinx/Vitis_Accel_Examples](https://github.com/Xilinx/Vitis_Accel_Examples)
						* [https://github.com/Xilinx/Vitis_Libraries](https://github.com/Xilinx/Vitis_Libraries)
						* [http://www.pynq.io/](http://www.pynq.io/)
					</textarea>
				</section>
				<section data-background-image="images/coffee.jpg">
					<h1>?</h1>
				</section>
				<section data-background-image="images/coffee.jpg">
					<h1>Thanks!</h1>
				</section>
			
			</section>	
			</section>
		</div>
	</div>

	<script src="revealjs/dist/reveal.js"></script>
	<script src="revealjs/plugin/notes/notes.js"></script>
	<script src="revealjs/plugin/markdown/markdown.js"></script>
	<script src="revealjs/plugin/highlight/highlight.js"></script>
	<script>
		// More info about initialization & config:
		// - https://revealjs.com/initialization/
		// - https://revealjs.com/config/
		Reveal.initialize({
			hash: true,

			// Learn about plugins: https://revealjs.com/plugins/
			plugins: [RevealMarkdown, RevealHighlight, RevealNotes]
		});
	</script>
	<script src="revealjs/plugin/math/math.js"></script>
	<script>
		Reveal.initialize({
			math: {
				mathjax: 'https://cdn.jsdelivr.net/gh/mathjax/mathjax@2.7.8/MathJax.js',
				config: 'TeX-AMS_HTML-full',
				// pass other options into `MathJax.Hub.Config()`
				TeX: { Macros: { RR: "{\\bf R}" } },
			},
			plugins: [RevealMath]
		});
	</script>
</body>

</html>